---
title: "OS Notes: Linux"
date: 2020-04-15T12:11:28+08:00
---

## Processes

### Impl of processes and threads

#### Processes

The Linux kernel internally represents processes as __tasks__, via the structure _task_struct_. Linux uses the task structure to represent any execution context.

For each process, a process descriptor of type _task\_struct_ is resident in memory at all times. It contains vital information needed for the kernel's management of all processes. The process descriptor along with memory for the kernel-mode stack for the process are created upon process creation.

For compatibility with other UNIX systems, Linux identifies processes via the PID. The kernel organizes all processes in a bidirectional linked-list of task structures. The PID can be mapped to the address of the task structure in order to access the info directly.

The task structure contains a variety of fields. Some of these fields contain pointers to other data structures or segments, which may be related to the user-level structure of the process, which is not interest when the user process is not runnable. Thereforce, these may be swapped or paged in order not to waste memory. However, info about signals must be in memory all the time, even when the process is not present in memory, while file descriptors can be kept in the user structure and brought in only when the process is in memory and runnable.

The info in the process descriptor falls into a number of broad categories that can be roughly described as follows:

1. __Scheduling parameters__ which are used to determine which process to run next.
2. __Memory image__ with pointers to the text, data, and stack segments, or page tables.
3. __Signals__.
4. __Machine registers__ are used to save registers when a trap to the kernel occurs.
5. __System call state__.
6. __File descriptor table__.
7. __Accounting__ as a pointer to a table that keeps track of the user and system CPU time used by the process and other info.
8. __Kernel stack__ is a fixed stack for use by the kernel part of the process.
9. __Miscellaneous__. Current process state, PID, parent process PID, user and group identification and so on.

When a __fork__ system call is executed, the calling process traps to the kernel and creates a task structure and few other accompanying data structures, such as kernel-mode stack and a _thread\_info_ structure. This structure is allocated at a __fixed__ offset from the process' end-of-stack, and contains few process parameters, along with the address of the process descriptor.

The majority of the process descriptor contents are filled out based on the parent's descriptor values. The system then looks for an available PID and updated the PID hash-table entry to point to the new task structure. It also sets the fileds in the _task\_struct_ to point to the previous/next process on the task array.

Here with the mechanism called __copy on write__, the child process is given its own page tables, but have them point to the parent's pages, only marked read only. Whenever either parent/child process tries to write on a page, it gets a protection fault. The kernel then allocates a new copy of the page to the faulting process and marks it read/write.

When a __exec__ system call is executed then, the kernel finds and verifies the executable file, copies the arguments and enviroment strings to the kernel, and releases the old address space and its page table. Now the new address space must be created and filled in. If the system supports mapped files, the new page tables are set up to indicate that no pages are in memory, except perhaps one stack page, but that the address space is backed by executable file on disk. When the new process starts running, it will get a page fault, which will cause the first page of code to be paged in from the executable file. In this way, nothing has to be loaded in advance, so programs can start quickly and fault in just those pages they need and no more. Finally, the arguments and environment strings are copied to the new stack, the signals are reset, and the registers are initialized to all zeros.

![image-20200416165952200](https://i.loli.net/2020/04/16/ZqVB3yaeovKOSUL.png)

#### Threads

Here just focus on kernel threds in Linux, particalarly on the differences among the Linux thread model and other Unix systems. There are several challenging decisions present.

Linux introduced a powerful new system call, __clone__, that blurred the distinction between processes and threads. The signature is as follws:

```c++
int clone(int (*fn)(void *), void *stack, int flags, void *arg);
// return pid
```

The _flags_ parameter is a bitmap that allows a finer grain of sharing than UNIX systems. Each of the bits can be set indelendently of the other ones.

![image-20200416173330358](https://i.loli.net/2020/04/16/kdcMIAUNq3BOWv6.png)

The fine-grained sharing is possible because Linux maintains separate structures for the various items. The task structure just points to these data structures, so it is easy to make a new task structure for each cloned thread and have it point either to the old thread's scheduling, memory, and other data structures or to copies of them.

To be compatible with othe UNIX systems, Linux distinguishes between PID and a task identifier (TID). Both fields are stored in the task structure. When _clone_ is used to create a new process that shares nothing with its creator, PID is set to a new value; otherwise the task receives a new TID, but inherits the PID.

### Scheduling

To start with, Linux threads are kernel threads, so scheduling is based on threads but not processes. Linux distinguishes three classes of threads for scheduling purposes:

1. Real-time FIFO.
2. Real-time round robin.
3. Timesharing.

Real-time FIFO threads are the highest prioroty and are not preemptable except by a newly ready real-time FIFO thread with higher priority. Real-time round robin threads are the same as real-time FIFO threads except that they have time quanta associated with them, and are preemptable by the clock. If multiple real-time round-robin threads are ready, each one is run for its quantum, after which it goes to the end of the list of real-time round-robin threads. Neither of these classes is actually real time in any sense. The reason they are called real time is that Linux is conformant to the P1003.4 standard which uses those names. These threads are internally represented with __priority__ levels from 0, which is the highest, to 99.

Non-real-time threads from a separate class and are scheduled by a separate algorithm. Internally, these threads are associated with priority levels from 100 to 139.

Like most UNIX systems, Linux associates a __nice__ value with each thread, which ranges from -20 to +19.

Linux scheduling algorithms are closely related to the design of the __runqueue__, a key data structure used by the scheduler to track all runnable tasks in the system and select the next one to run. A runqueue is associated with each CPU in the system.

A popular Linux scheduler was the Linux __O(1) scheduler__. The runqueue is organized in two arrays, _active_ and _expired_. Each of these is an array of 140 list heads, each correpsonding to a different priority. Each list head points a doubly linked-list of processes at a given priority. The basic operation can be described as follows.

The scheduler selects a task from the highest-priority list in the active array. If that task's quantum (timeslice) expires, it is moved to the expired list (potentially at a different priority level). If the task blocks, for instance to wait on an I/O event, before its timeslice expires, once the event occurs and its excution can resume, it is placed back on the original active array, and its timeslice is decremented to reflect the CPU time it already used. Once its timesplice is fully exhausted, it, too, will be placed on the expire array. When there are no more tasks in the active array, the scheduler simply sways the pointers, so the expired arrays now become active, and vice versa.

Here different priority levels are assigned different timeslice values, with higher quanta assigned to higher-priority processes.

Since Linux does not know a priori whether a task is I/O- or CPU-bound, it relies on continuously maintaing interactivity heuristics. In this manner, Linux distinguishes between static and dynamic priority. The threads' dynamic priority is continuously recalculated, so as to (1) reward interactive threads, and (2) punish CPU-hogging threads. The scheduler maintains a _sleep\_avg_ variable associated with each task. Whenever a task is awakened, this variable is incremented. Whenever a task is preempted or when its quantum expires, this variable is decremented by the corresponding value. This value is used to dynamically map the task's bonus to values from -5 to +5. The scheduler recalculates the new priority level when a thread is moved from the active to the expired list.

![image-20200418020721965](https://i.loli.net/2020/04/18/PSDO54qKJZdQnox.png)

Most notably, the heuristics used to determine the interactivity of a task, and therefore its priority level, were complex and imperfect, and resulted in poor performance for interactive tasks. To address the issue, a new scheduler called __Completely Fair Scheduler (CFS)__ is proposed.

The main idea is to use a _red-black tree_ as the runqueue data structure. Tasks are ordered in the tree based on the amount of time they spend running on the CPU, called _vruntime_. CFS accounts for the tasks' running time with nanosecond granularity. Each internal node in the tree corresponds to a task. The children to the left correspond to tasks which had less time on the CPU, and therefore will be scheduled sooner, and the children to the right on the node are those that have consumed more CPU time thus far.

The scheduling algorithm can be summarized as follows. CFS allows schedules the task which has had least amount of time on the CPU, typically the leftmost node in the tree. Periodically, CFS increments the task's _vruntime_ value base on the time it has already run, and compares this to the current leftmost node in the tree. If the running task still has the smaller _vruntime_, it will continue to run. Otherwise, it will be inserted at the appropriate place in the red-black tree, and the CPU will given to task corresponding to the new leftmost node.

To account for differences in task priorities and niceness, CFS changes the effictive rate at which a task's virtual time passes when it is running on the CPU. For lower-priority tasks, time passes more quickly, and they will lose the CPU and be reinserted in the stree sooner. In this manner, CFS avoids using separate runqueue structures for different priority levels.

The Linux scheduler includes special features particularly useful for multiprocessor or multicore platforms.

1. The runqueue structure is associated with each CPU. The scheduler tries to maintain benefits from affinity scheduling, and to schedule tasks on the CPU on which they were previously executing.
2. A set of system calls is available to further specify or modify the affinity requirements of a select thread.
3. The scheduler performs periodic load balancing across runqueues of different CPUs to ensure that the system load is well balanced, while still meeting certain performance or affinity requirements.

Tasks which are not runnable and are waiting on various I/O operations or other kernel events are placed on another data structure, __waitqueue__. A waitqueue is associated with each event that tasks may wait on.  The head of the quue includes a pointer to a linked list of tasks and a spinlock. The spinlock is necessary so as to ensure that the waitqueue can be concurrently manipulated through both the main kernel code and interrupt handlers or other async invocations.

### Booting

When the computer starts, the BIOS performs Power-On-Self-Test (POST) and initial device discovery and initialization, since the OS' boot process may rely on access to disks, screens, keyboards, and so on. Next, the first sector of the boot disk, the __MBR (Master Boot Record)__, is read into a fixed memory location and executed. This sector contains a small program that loads a standalone program called __boot__ from the boot device. The _boot_ program first copies itself to a fixed high-memory address to free up low memory for the os. Once moved, _boot_ reads the root directory of the boot device. To do this, it must understand the file system and directory format, which is the case with some bootloaders such as __GRUB (GRand Unified Bootloader)__. They need a block map and low-level addresses, which describe physical sectors, heads, and cylinders, to find the relevant sectors to be loaded. Then _boot_ reads in the os kernel and jumps to it. At this point, it has finished its job and the kernel is running.

The kernel start-up code is written in assembly language and is highly machine dependent. Typical work includes setting up the kernel stack, identifying the CPU type, calculating the amount of RAM present, disable interrupts, enabling the MMU, and finally calling the C-language _main_ procedure to start the main part of the os.

The C code also has considerable initialization to do, but this is more logical than physical. It starts out by alocating a message buffer to help debug boot problems. Next the kernel data structures are allocated. A few of them, such as the page cache and certain page table structures, depend on the amount of RAM avialable. At the point the system begins autoconfiguration. Using configuration files teling what kinds of I/O devices might be present, it begins probing the devices to see which ones actually are present. If a probed device responds to the probe, it is added to a table of attached devices. Unlike traditional UNIX versions, Linux device drivers do not need to be statically linked and may be loaded dynamically.

Once all the hardware has been configured, the next thing to do is to carefully handcraft process 0, set up its stack, and run it. Process 0 continues initialization, doing things like programming the real-time clock, mounting the root file system, and creating _init_ (process 1) and the page daemon (process 2).

_Init_ checks its flags to see if it is supposed to come up single user or multiuser. In the former case, it forks off a process that executes the shell and waits for this process to exit. In the latter case, it forks off a process that executes the system initialization shell script which can do file system consistency checks, mount additional file systems, start daemon processes, and so on.

